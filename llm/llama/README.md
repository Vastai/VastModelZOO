# LLaMA

- Technical Report
    - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- Huggingface
    - https://huggingface.co/meta-llama


## Model Arch

è‡ªä»OpenAIæ¨å‡ºChat GPTç³»åˆ—åï¼Œä¹Ÿæ ‡å¿—ç€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘â€”â€”å¤§æ¨¡å‹LLMï¼ˆLarge Language Modelï¼‰çš„çˆ†ç«ã€‚å¼€æºç¤¾åŒºçš„å…ˆé”‹å½“å±Metaå…¬å¸æ¨å‡ºçš„LLAMA(Large Language Model Meta AI)ç³»åˆ—ï¼Œä½œä¸ºDecoder-Onlyç»“æ„çš„ä»£è¡¨ï¼Œä¸ºåç»­å¼€æºå¤§æ¨¡å‹æŒ‡æ˜äº†æ–¹å‘ã€‚

![llama_arch](../../images/llm/llama/llama_arch.png)


### LLaMA v1
- é«˜è´¨é‡æ•°æ®é›†
    - é¢„è®­ç»ƒæ•°æ®å¤§çº¦åŒ…å« 1.4T tokens
    - æ•°æ®å¤„ç†æ­¥éª¤ï¼šç­›é€‰ä½è´¨é‡æ•°æ®ã€æ•°æ®å»é‡ã€æ•°æ®å¤šæ ·æ€§
- å½’ä¸€åŒ–ä¼˜åŒ–ï¼Œå¢åŠ Pre-Normalizationæ­¥éª¤ï¼Œä½¿ç”¨RMSNormä»£æ›¿LayerNormï¼Œç®€åŒ–è®¡ç®—ï¼Œæå‡æ•ˆç‡
- æ¿€æ´»å‡½æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨SwiGLUä»£æ›¿ReLUä»¥æé«˜æ€§èƒ½
- Rotary Embeddingsæ—‹è½¬ä½ç½®ç¼–ç ï¼ŒRoPEæ—‹è½¬ä½ç½®ç¼–ç çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œé€šè¿‡ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç â€ï¼Œè¿™ä¸€æ„æ€å…·å¤‡äº†ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹ä¾¿æ€§ï¼ŒåŒæ—¶å¯ä»¥è¡¨ç¤ºä¸åŒtokenä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»


## Build_In Deploy

### step.1 æ¨¡å‹å‡†å¤‡

1. ä¸‹è½½æ¨¡å‹æƒé‡

    | models  | tips |
    | :--- | :--: | 
    | [alexl83/LLaMA-33B-HF](https://huggingface.co/alexl83/LLaMA-33B-HF)   | MHA |

    > - å…¶å®ƒåŸºäº`llama`å¾®è°ƒçš„æ¨¡å‹(`model_type:llama`)ï¼Œè½¬æ¢åŠæ¨ç†æµ‹è¯•å‚è€ƒ`llama`ç³»åˆ—å³å¯
    > - `meta-llama`å¼€æºçš„æ¨¡å‹å‡ä¸æ”¯æŒå•†ç”¨ï¼Œè¯·æŸ¥é˜…åŸå§‹è®¸å¯è¯


### step.2 æ•°æ®é›†

1. é‡åŒ–æ ¡å‡†æ•°æ®é›†ï¼š
    - [allenai/c4](https://hf-mirror.com/datasets/allenai/c4/tree/main/en)
        - c4-train.00000-of-01024.json.gz
        - c4-validation.00000-of-00008.json.gz
    - [ceval/ceval-exam](https://hf-mirror.com/datasets/ceval/ceval-exam/tree/main)
        - ceval-exam.zip
    - [yahma/alpaca-cleaned](https://hf-mirror.com/datasets/yahma/alpaca-cleaned/tree/main)
        - alpaca_data_cleaned.json

### step.3 æ¨¡å‹è½¬æ¢

1. æ ¹æ®å…·ä½“æ¨¡å‹ï¼Œä¿®æ”¹æ¨¡å‹è½¬æ¢é…ç½®æ–‡ä»¶
    - v1/v2/v3æ¨¡å‹ï¼Œç¼–è¯‘é…ç½®ä¸€è‡´
    - [hf_llama_fp16.yaml](./build_in/build/hf_llama_fp16.yaml)
    - [hf_llama_int8.yaml](./build_in/build/hf_llama_int8.yaml)

    > - runstreamæ¨ç†ï¼Œç¼–è¯‘å‚æ•°`backend.type: tvm_vacc`
    > - fp16ç²¾åº¦: ç¼–è¯‘å‚æ•°`backend.dtype: fp16`
    > - int8ç²¾åº¦: ç¼–è¯‘å‚æ•°`backend.dtype: int8`
    
    ```bash
    vamc compile ./build_in/build/hf_llama_fp16.yaml
    vamc compile ./build_in/build/hf_llama_int8.yaml
    ```

### step.4 æ¨¡å‹æ¨ç†
1. å‚è€ƒå¤§æ¨¡å‹éƒ¨ç½²æ¨ç†å·¥å…·ï¼š[vastgenx: v1.1.0+](../../tools/vastgenx/README.md)

### Tips
- **LLMæ¨¡å‹è¯·å…ˆæŸ¥çœ‹æ¦‚è¦æŒ‡å¼•**ï¼Œ[TipsğŸ””](../README.md)
- llamaç³»åˆ—ï¼Œä¸ä¼šå¯¹åŸå§‹llama_modeling.pyè¿›è¡Œä¿®æ”¹ï¼Œä¸ºå…¼å®¹å¤šç‰ˆæœ¬æ¨¡å‹ï¼Œå»ºè®®ä¾èµ–é…ç½®å¦‚ä¸‹ï¼š
    ```bash
    protobuf==3.20.3
    torch==2.1.0
    onnx==1.14.0
    onnxsim==0.4.28
    onnxruntime==1.13.1
    accelerate==0.25.0
    transformers==4.34.0
    ```
