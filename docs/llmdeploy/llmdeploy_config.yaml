
model:
  ################# commom #######################
  model_name: Llama-2-7b-hf-tp16-b2s-fp16
  batch_size: 16
  prompt_template: "You are a helpful assistant. You do not respond as User or pretend to be User. You only respond once as Assistant. Human: {}  \nAssistant: "
  # model_deploy_config: /home/mqxu/work/project/release_ai161/ai161sp4/tp4/baichuan2_7b_norm-int8-gptq-dynamic-vacc/baichuan2_7b_norm_deploy.json
  # model_deploy_config: /home/rzhang/models/baichuan2_13b-fp16-none-dynamic-vacc/baichuan2_13b_deploy.json
  # model_deploy_config: /media/release/ai_1.6.2.SP1/Llama-2-7b-hf-fp16-b2s-tp2-none-dynamic-vacc/Llama-2-7b-hf_deploy.json
  model_deploy_config: /media/release/ai_1.6.2.RC2/Llama-2-7b-hf-tp16-b2s-fp16-none-dynamic-vacc/Llama-2-7b-hf-tp16-b2s_deploy.json
  ################# commom #######################

  ################# model type #######################
  model_infer_type: tp
  publish_port: 77810
  subscribe_port: 77110
  tp_device_ids_list: [0,1,2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  tp_elf_path_list: [/home/wzp/code/llm/llmdeploy/0306/llmdeploy/3rdparty/elf/0222gpt_tp_b2s_stage_2] #0205gpt_tp_b2s_stage_2]


  ################# model type #######################

  ################# decode #######################
  top_k: 1
  top_p: 1.0
  temperature: 0.0
  max_answer_tokens: 2048
  ################# decode #######################


api:
  server_host: 0.0.0.0
  server_port: 7863

webui:
  server_host: 0.0.0.0
  server_port: 8502 #8501
  webui_image_path: ./docs
  with_prompt: False #True
  max_input_size: 10240
  model_deploy_lists: [/media/release/ai_1.6.2.RC2/Llama-2-7b-hf-tp16-b2s-fp16-none-dynamic-vacc/Llama-2-7b-hf-tp16-b2s_deploy.json]


benchmark:
  use_api: False
  input_output_len: [512,512]
  test_batch_sizes: [16,32,64]
  test_counts: 100
  card_type: VA1L
  # test_txt: /home/mqxu/work/project/llmdeploy/result/ShareGPT_V3_unfiltered_cleaned_split.json
  save_result: ./result/

eval:
  config_path: ./configs/eval/eval_demo.py
  save_result: ./configs/eval/outputs/default
  datasets_dir: ../
  dry_run: False # 只查看任务切分
  eval_mode: all # infer eval viz
