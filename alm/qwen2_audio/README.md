# Qwen2-Audio

- blog: https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list
- report: https://arxiv.org/abs/2407.10759
- code: https://github.com/QwenLM/Qwen2-Audio
- huggingface: https://huggingface.co/Qwen/Qwen2-Audio-7B
- modelscope: https://www.modelscope.cn/models/Qwen/Qwen2-Audio-7B


## Model Arch

```mermaid
---
config:
  theme: base
  themeVariables:
    primaryColor: "#4f46e5"
    edgeLabelBackground: "#ffffff"
    tertiaryColor: "#f1f5f9"
---
flowchart TB
    %% --- æ ·å¼å®šä¹‰ ---
    classDef inputNode fill:#1e293b,stroke:#334155,stroke-width:2px,color:#f8fafc
    classDef audioNode fill:#8b5cf6,stroke:#7c3aed,stroke-width:1px,color:#ffffff
    classDef llmNode fill:#4f46e5,stroke:#4338ca,stroke-width:1px,color:#ffffff
    classDef opNode fill:#6366f1,stroke:#4f46e5,stroke-width:1px,color:#ffffff,font-style:italic
    classDef shapeNode fill:#f8fafc,stroke:#e2e8f0,stroke-width:1px,color:#475569,font-family:ui-monospace,font-size:12px
    classDef finalNode fill:#0ea5e9,stroke:#0284c7,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef modelNode fill:#ffffff,stroke:#1e293b,stroke-width:2px,color:#1e293b

    %% --- 1. è¾“å…¥å±‚ (Input Stage) ---
    subgraph Stage_Input ["<b>ğŸ“¥ å¤šæ¨¡æ€è¾“å…¥å±‚ (Multimodal Input)</b>"]
        direction TB
        Audio_IN["ğŸ¤ åŸå§‹éŸ³é¢‘<br/>16kHz | 25msçª—å£ | 10msè·³æ­¥"]:::inputNode
        Text_IN["ğŸ“ æ–‡æœ¬/æŒ‡ä»¤<br/>å« <audio> Token (ID: 151646)"]:::inputNode
    end

    %% --- 2. éŸ³é¢‘ç¼–ç å™¨ (Whisper-large-v3 based) ---
    subgraph Stage_Audio ["<b>ğŸµ éŸ³é¢‘ç¼–ç å™¨ (Audio Encoder)</b>"]
        direction TB
        
        subgraph Audio_Pre ["ç‰¹å¾é¢„å¤„ç†"]
            A1["Mel-Spectrogram<br/>128é€šé“ | 40ms/å¸§"]:::opNode
            A12["Log-Mel Spectrogram<br/>(128, 3000)"]:::shapeNode
            A2["CNNä¸‹é‡‡æ ·<br/>Conv1dÃ—2 (stride=2)"]:::opNode
        end
        
        subgraph Audio_Trans ["Whisper Transformer Ã— 32"]
            A3["Encoder Layer<br/>20-head Attn + GELU"]:::modelNode
            A4["[Output]<br/>(batch, T/2, 1280)"]:::shapeNode
        end
        
        Audio_Pre --> Audio_Trans
    end

    %% --- 3. æŠ•å½±ä¸ç‰¹å¾å¯¹é½ (Projection) ---
    subgraph Stage_Align ["<b>ğŸ”— è·¨æ¨¡æ€æŠ•å½±å¯¹é½ (Projector)</b>"]
        direction TB
        P1["Linear Projection<br/>1280 â” 4096"]:::opNode
        P2["ç‰¹å¾æ± åŒ–/æ’å€¼<br/>Pooling Stride=2 é€‚é…"]:::opNode
        P3["Audio Token Embeddings<br/>(batch, audio_len, 4096)"]:::shapeNode
        P1 --> P2 --> P3
    end

    %% --- 4. è¾“å…¥å¤„ç†ä¸åˆå¹¶ (Input Processing) ---
    subgraph Stage_Merge ["<b>ğŸ§© è¾“å…¥åºåˆ—æ„å»º (Input Stitching)</b>"]
        direction TB
        T1["æ–‡æœ¬ Tokenization<br/>Vocab=156032 | Qwen2Tokenizer"]:::opNode
        T2["è·å–æ–‡æœ¬åµŒå…¥<br/>(batch, text_len, 4096)"]:::shapeNode
        M1["merge_input_ids_with_image_features<br/>æ›¿æ¢ <audio> placeholder"]:::opNode
        M2["ä½ç½®ç¼–ç æ³¨å…¥<br/>RoPE (theta=10000)"]:::opNode
        M3["æ³¨æ„åŠ›æ©ç æ„å»º<br/>Sliding Window=32K"]:::opNode
        T1 --> T2 --> M1 --> M2 --> M3
    end

    %% --- 5. è¯­è¨€æ¨¡å‹å‰å‘ä¼ æ’­ (Qwen2-7B) ---
    subgraph Stage_LLM ["<b>ğŸ§  å¤§è¯­è¨€æ¨¡å‹ (Qwen2-7B Backbone)</b>"]
        direction LR
        L1["Input Embedding<br/>(éŸ³é¢‘+æ–‡æœ¬æ··åˆåºåˆ—)"]:::shapeNode
        L2["Decoder Layers Ã— 32<br/>GQA + RMSNorm + SwiGLU"]:::modelNode
        L3["Final RMSNorm<br/>(eps=1e-5)"]:::opNode
        L4["LM Head<br/>Logits: (batch, seq, 156032)"]:::shapeNode
        L1 --> L2 --> L3 --> L4
    end

    %% --- 6. è¾“å‡ºç”Ÿæˆ (Output) ---
    subgraph Stage_Output ["<b>ğŸ’¬ è¾“å‡ºç”Ÿæˆ (Generation)</b>"]
        direction LR
        GEN["Auto-regressive Generation<br/>Sample / Greedy / Beam"]:::finalNode
        OUT["æ–‡æœ¬å“åº”<br/>ASR/S2TT/åˆ†æ/å¯¹è¯"]:::finalNode
        GEN --> OUT
    end

    %% --- å…¨å±€è¿æ¥ ---
    Audio_IN --> A1
    Text_IN --> T1
    A4 --> P1
    P3 --> M1
    T2 -.-> M1
    M3 --> L1
    L4 --> GEN

    %% æ ·å¼ä¿®é¥°
    style Stage_Audio fill:#f5f3ff,stroke:#8b5cf6,stroke-dasharray: 5 5
    style Stage_LLM fill:#eff6ff,stroke:#4f46e5,stroke-width:2px
    style Stage_Align fill:#f0fdf4,stroke:#10b981
    style Stage_Merge fill:#fffbeb,stroke:#f59e0b
```

### å±‚çº§ç»“æ„è§£æ

```cmd
Qwen2AudioForConditionalGeneration (Multimodal: Audio+Text â†’ Text)
â”œâ”€â”€ Qwen2AudioEncoder (Whisper-large-v3 based, 325M params)
â”‚   â”œâ”€â”€ conv1 (Conv1d: 128 mel-bins â†’ 1280, kernel=3, stride=1)
â”‚   â”œâ”€â”€ embed_positions (Embedding: 1500Ã—1280 learnable)
â”‚   â”œâ”€â”€ encoder (32Ã— Qwen2AudioEncoderLayer)
â”‚   â”‚   â”œâ”€â”€ self_attn (Multi-Head Attention: 20 heads, d_k=64)
â”‚   â”‚   â”œâ”€â”€ self_attn_layer_norm (LayerNorm: 1280)
â”‚   â”‚   â”œâ”€â”€ fc1 (Linear: 1280 â†’ 5120, GELU activation)
â”‚   â”‚   â”œâ”€â”€ fc2 (Linear: 5120 â†’ 1280)
â”‚   â”‚   â””â”€â”€ final_layer_norm (LayerNorm: 1280)
â”‚   â””â”€â”€ avg_pooler (Temporal pooling: stride=2, 40ms/frame â†’ 80ms/frame)
â”‚
â”œâ”€â”€ multi_modal_projector (Audio-to-Language Bridge)
â”‚   â””â”€â”€ linear (Linear: 1280 â†’ 4096, GELU)
â”‚
â””â”€â”€ language_model (Qwen2-7B, Decoder-only, 7.6B params)
    â”œâ”€â”€ embed_tokens (Embedding: 156032 Ã— 4096, incl. audio_token 151646)
    â”œâ”€â”€ layers (32Ã— Qwen2DecoderLayer)
    â”‚   â”œâ”€â”€ self_attn (GQA: 32 query heads / 8 kv heads, head_dim=128)
    â”‚   â”‚   â”œâ”€â”€ q_proj (Linear: 4096 â†’ 4096)
    â”‚   â”‚   â”œâ”€â”€ k_proj (Linear: 4096 â†’ 1024)  # GQAå‹ç¼©
    â”‚   â”‚   â”œâ”€â”€ v_proj (Linear: 4096 â†’ 1024)  # GQAå‹ç¼©
    â”‚   â”‚   â””â”€â”€ o_proj (Linear: 4096 â†’ 4096)
    â”‚   â”œâ”€â”€ mlp (SwiGLU architecture)
    â”‚   â”‚   â”œâ”€â”€ gate_proj (Linear: 4096 â†’ 11008)
    â”‚   â”‚   â”œâ”€â”€ up_proj (Linear: 4096 â†’ 11008)
    â”‚   â”‚   â””â”€â”€ down_proj (Linear: 11008 â†’ 4096)
    â”‚   â”œâ”€â”€ input_layernorm (RMSNorm: 4096, eps=1e-5)
    â”‚   â””â”€â”€ post_attention_layernorm (RMSNorm: 4096)
    â””â”€â”€ norm (RMSNorm: 4096)
    
lm_head (Linear: 4096 â†’ 156032, weight tied with embed_tokens)
```

**å…³é”®ç»´åº¦å˜åŒ–æµ**ï¼š

- éŸ³é¢‘è¾“å…¥: `(batch, 128, 3000)` [mel-spectrogram, 30s audio]
- ç¼–ç å: `(batch, 1500, 1280)` [temporal downsampling by 2]
- æŠ•å½±å: `(batch, 750, 4096)` [Audio Tokens]
- æ–‡æœ¬åµŒå…¥: `(batch, seq_len, 4096)`
- èåˆåºåˆ—: `(batch, 750+seq_len, 4096)` â†’ Decoder â†’ Logits `(batch, total_len, 156032)`

### æ•°æ®æµè¯¦è§£

**é˜¶æ®µä¸€ï¼šéŸ³é¢‘ç‰¹å¾æå–**
åŸå§‹éŸ³é¢‘ï¼ˆ16kHzé‡‡æ ·ç‡ï¼‰é¦–å…ˆç»è¿‡128é€šé“Mel-Spectrogramå˜æ¢ï¼Œçª—å£å¤§å°25msï¼Œè·³æ­¥10msï¼Œæ¯å¸§å¯¹åº”åŸå§‹éŸ³é¢‘çº¦40msã€‚éšåé€šè¿‡ä¸¤å±‚Conv1dè¿›è¡Œä¸‹é‡‡æ ·ï¼ˆstride=2ï¼‰ï¼Œæ—¶é—´åˆ†è¾¨ç‡å‹ç¼©ä¸ºåŸå§‹é•¿åº¦çš„1/4ã€‚Whisper-large-v3ç¼–ç å™¨ï¼ˆ32å±‚Transformerï¼Œ20å¤´æ³¨æ„åŠ›ï¼Œd_model=1280ï¼‰å¤„ç†åçš„ç‰¹å¾ç»´åº¦ä¸º(batch, audio_seq_len, 1280)ï¼Œå¯¹äº30ç§’éŸ³é¢‘ï¼Œseq_lençº¦ç­‰äº750ã€‚

**é˜¶æ®µäºŒï¼šè·¨æ¨¡æ€æŠ•å½±**
éŸ³é¢‘ç‰¹å¾é€šè¿‡å•å±‚çº¿æ€§æŠ•å½±ï¼ˆ1280â†’4096ï¼Œæ— biasï¼‰æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´ã€‚è¿™ä¸€æç®€å¯¹é½ç­–ç•¥ï¼ˆä»…5.2Må¯å­¦ä¹ å‚æ•°ï¼‰é¿å…äº†å¤æ‚æŠ•å½±å±‚ï¼ˆå¦‚Q-Formerï¼‰å¸¦æ¥çš„ä¿¡æ¯ç“¶é¢ˆï¼ŒåŒæ—¶ä¿ç•™äº†Whisperç¼–ç å™¨é¢„è®­ç»ƒè·å¾—çš„é²æ£’æ€§éŸ³é¢‘è¡¨å¾ã€‚

**é˜¶æ®µä¸‰ï¼šåºåˆ—æ„å»ºä¸ä½ç½®ç¼–ç **
æ–‡æœ¬è¾“å…¥é€šè¿‡Tokenizerï¼ˆè¯è¡¨å¤§å°156,032ï¼‰è½¬æ¢ä¸ºtoken IDsï¼Œå…¶ä¸­ç‰¹æ®Štoken `<audio>`ï¼ˆID: 151646ï¼‰ä½œä¸ºéŸ³é¢‘å ä½ç¬¦ã€‚åœ¨è¾“å…¥å±‚ï¼Œ`<audio>` tokençš„embeddingè¢«æ›¿æ¢ä¸ºæŠ•å½±åçš„éŸ³é¢‘ç‰¹å¾ï¼Œå½¢æˆæ··åˆåºåˆ—ã€‚RoPEä½ç½®ç¼–ç ï¼ˆÎ¸=10000ï¼‰éšååº”ç”¨äºQueryå’ŒKeyå¼ é‡ï¼Œä¸ºæ¨¡å‹æä¾›ç›¸å¯¹ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ã€‚

**é˜¶æ®µå››ï¼šTransformerè§£ç **
æ··åˆåºåˆ—ç»è¿‡32å±‚Qwen2DecoderLayerå¤„ç†ï¼Œæ¯å±‚åŒ…å«ï¼š

1. Pre-Attention RMSNormï¼ˆeps=1e-5ï¼‰ç¨³å®šè¾“å…¥åˆ†å¸ƒ
2. GQAæ³¨æ„åŠ›æœºåˆ¶ï¼ˆ32 Queryå¤´ï¼Œ4 Key/Valueå¤´ï¼‰ï¼Œé€šè¿‡å¹¿æ’­æœºåˆ¶å®ç°8å€KVç¼“å­˜å‹ç¼©
3. Post-Attention RMSNorm
4. SwiGLUå‰é¦ˆç½‘ç»œï¼ˆintermediate_size=11008ï¼Œçº¦2.68å€æ‰©å±•æ¯”ï¼‰

**é˜¶æ®µäº”ï¼šè‡ªå›å½’ç”Ÿæˆ**
æœ€ç»ˆéšè—çŠ¶æ€ç»LM Headï¼ˆ4096â†’156032çº¿æ€§æŠ•å½±ï¼‰è½¬æ¢ä¸º logitsï¼Œé€šè¿‡é‡‡æ ·ç­–ç•¥ï¼ˆtemperature=0.7, top_p=0.9ï¼‰ç”Ÿæˆæ–‡æœ¬å“åº”ï¼Œæ”¯æŒASRã€ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€è‡ªç”±å¯¹è¯ç­‰å¤šç§ä»»åŠ¡ã€‚



## Training

### Training Arch

```mermaid
---
config:
  theme: base
  themeVariables:
    primaryColor: "#4f46e5"
    edgeLabelBackground: "#ffffff"
    tertiaryColor: "#f1f5f9"
---
flowchart LR
    %% --- æ ·å¼å®šä¹‰ ---
    classDef inputNode fill:#1e293b,stroke:#334155,stroke-width:2px,color:#f8fafc
    classDef pretrainNode fill:#3b82f6,stroke:#2563eb,stroke-width:1px,color:#ffffff
    classDef sftNode fill:#22c55e,stroke:#16a34a,stroke-width:1px,color:#ffffff
    classDef dpoNode fill:#f59e0b,stroke:#d97706,stroke-width:1px,color:#ffffff
    classDef opNode fill:#6366f1,stroke:#4f46e5,stroke-width:1px,color:#ffffff,font-style:italic
    classDef modelNode fill:#ffffff,stroke:#1e293b,stroke-width:2px,color:#1e293b
    classDef shapeNode fill:#f8fafc,stroke:#e2e8f0,stroke-width:1px,color:#475569,font-family:ui-monospace,font-size:12px
    classDef finalNode fill:#0ea5e9,stroke:#0284c7,stroke-width:2px,color:#ffffff,font-weight:bold

    %% --- é˜¶æ®µä¸€ï¼šMulti-Task Pre-training ---
    subgraph Stage1 ["<b>ğŸ“š é˜¶æ®µä¸€ï¼šMulti-Task Pre-training</b><br/><span style='font-size:12px'>æ•°æ®é©±åŠ¨çš„åŸºç¡€èƒ½åŠ›æ„å»º</span>"]
        direction TB
        
        subgraph Pretrain_Input ["æ•°æ®è¾“å…¥å±‚"]
            P0["å¤šä»»åŠ¡è¯­æ–™åº“<br/>ASR + S2TT + VAD + Speaker ID"]:::inputNode
            P1["è‡ªç„¶è¯­è¨€æç¤ºå·¥ç¨‹<br/>æ›¿ä»£ä¼ ç»Ÿåˆ†å±‚æ ‡ç­¾<br/>Zero-Shot/Few-Shot Template"]:::opNode
        end
        
        subgraph Pretrain_Arch ["æ¶æ„åˆå§‹åŒ–"]
            P2["Whisper-large-v3 Encoder<br/>32å±‚ | 1280ç»´ | 20å¤´æ³¨æ„åŠ›"]:::modelNode
            P3["Qwen2-7B LLM Backbone<br/>32å±‚ | 4096ç»´ | GQA"]:::modelNode
            P4["Projection Layer<br/>éšæœºåˆå§‹åŒ– | 1280â†’4096"]:::shapeNode
        end
        
        subgraph Pretrain_Strategy ["è®­ç»ƒç­–ç•¥"]
            P5["ä¸‰é˜¶æ®µæ¸è¿›è§£å†»<br/>1ï¸âƒ£ ä»…æŠ•å½±å±‚ | 2ï¸âƒ£ åŠ å…¥Encoder<br/>3ï¸âƒ£ å…¨å‚æ•°å¾®è°ƒ"]:::opNode
            P6["Output<br/>è·¨æ¨¡æ€å¯¹é½è¡¨å¾"]:::finalNode
        end
        
        P0 --> P1 --> P2 --> P4
        P3 --> P4
        P4 --> P5 --> P6
    end

    %% --- é˜¶æ®µäºŒï¼šSupervised Fine-Tuning ---
    subgraph Stage2 ["<b>ğŸ¯ é˜¶æ®µäºŒï¼šSupervised Fine-Tuning (SFT)</b><br/><span style='font-size:12px'>äº¤äº’èƒ½åŠ›å¯¹é½ä¸æŒ‡ä»¤éµå¾ª</span>"]
        direction TB
        
        subgraph SFT_Data ["é«˜è´¨é‡æŒ‡ä»¤æ•°æ®"]
            S0["éŸ³é¢‘åˆ†ææŒ‡ä»¤<br/>ç¯å¢ƒè¯†åˆ« | æƒ…æ„Ÿæ£€æµ‹ | äº‹ä»¶åˆ†ç±»"]:::inputNode
            S1["è¯­éŸ³å¯¹è¯æ•°æ®<br/>Natural QA | çŸ¥è¯† grounded"]:::inputNode
            S2["åŒæ¨¡æ€äº¤ç»‡<br/>éŸ³é¢‘+æ–‡æœ¬ä¸Šä¸‹æ–‡"]:::shapeNode
        end
        
        subgraph SFT_Process ["è”åˆè®­ç»ƒ"]
            S3["éšå¼æ¨¡å¼è¯†åˆ«<br/>è‡ªåŠ¨åŒºåˆ†: è½¬å½•/ç¿»è¯‘/åˆ†æ/å¯¹è¯"]:::opNode
            S4["ç«¯åˆ°ç«¯å¾®è°ƒ<br/>å…¨å‚æ•°å¯è®­ç»ƒ<br/>å­¦ä¹ ç‡: 2e-5 | Cosine Decay"]:::opNode
            S5["æ³¨æ„åŠ›æ©ç ä¼˜åŒ–<br/>Sliding Window: 32K | å…¨å±€éŸ³é¢‘æ„ŸçŸ¥"]:::opNode
        end
        
        subgraph SFT_Output ["èƒ½åŠ›è¾“å‡º"]
            S6["SFT Checkpoint<br/>å…·å¤‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›"]:::finalNode
        end
        
        S0 --> S2
        S1 --> S2
        S2 --> S3 --> S4 --> S5 --> S6
    end

    %% --- é˜¶æ®µä¸‰ï¼šDirect Preference Optimization ---
    subgraph Stage3 ["<b>ğŸ­ é˜¶æ®µä¸‰ï¼šDirect Preference Optimization (DPO)</b><br/><span style='font-size:12px'>äººç±»åå¥½å¼ºåŒ–ä¸äº‹å®å¯¹é½</span>"]
        direction TB
        
        subgraph DPO_Data ["åå¥½å¯¹æ„å»º"]
            D0["Chosen vs Rejected Pairs<br/>ç›¸åŒè¾“å…¥ï¼Œä¸åŒè´¨é‡å“åº”"]:::inputNode
            D1["åå¥½ç»´åº¦è®¾è®¡<br/>äº‹å®å‡†ç¡®æ€§ > å®‰å…¨æ€§ > æœ‰ç”¨æ€§<br/>æµç•…åº¦ > ç®€æ´æ€§"]:::opNode
        end
        
        subgraph DPO_Algorithm ["ä¼˜åŒ–ç®—æ³•"]
            D2["æ— éœ€å¥–åŠ±æ¨¡å‹<br/>ç›´æ¥ç­–ç•¥ä¼˜åŒ–<br/>Î²=0.1 | Reference Model: SFT"]:::modelNode
            D3["KLæ•£åº¦çº¦æŸ<br/>é˜²æ­¢æ¨¡å‹åç¦»å¤ªè¿œ"]:::opNode
            D4["Branched Rollout<br/>æ‰¹æ¬¡å†…æ„å»ºæ­£è´Ÿæ ·æœ¬å¯¹"]:::shapeNode
        end
        
        subgraph DPO_Result ["æœ€ç»ˆè¾“å‡º"]
            D5["DPO Checkpoint<br/>äººç±»åå¥½å¯¹é½"]:::finalNode
            D6["éƒ¨ç½²å°±ç»ªæ¨¡å‹<br/>Qwen2-Audio-Instruct"]:::finalNode
        end
        
        D0 --> D1 --> D2
        S6 -.->|Reference Model| D2
        D2 --> D3 --> D4 --> D5 --> D6
    end

    %% --- å…¨å±€è¿æ¥ä¸æµç¨‹ ---
    Stage1 --> Stage2 --> Stage3
    
    %% --- æ ·å¼ä¿®é¥° ---
    style Stage1 fill:#eff6ff,stroke:#3b82f6,stroke-width:2px
    style Stage2 fill:#f0fdf4,stroke:#22c55e,stroke-width:2px
    style Stage3 fill:#fffbeb,stroke:#f59e0b,stroke-width:2px
    style Pretrain_Input fill:#dbeafe,stroke:#60a5fa,stroke-dasharray: 3 3
    style SFT_Data fill:#dcfce7,stroke:#4ade80,stroke-dasharray: 3 3
    style DPO_Data fill:#fef3c7,stroke:#fbbf24,stroke-dasharray: 3 3
```

Qwen2-Audio é‡‡ç”¨äº†**ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼**ï¼ˆPre-training â†’ Supervised Fine-Tuning â†’ Direct Preference Optimizationï¼‰ï¼Œçªç ´äº†ä¼ ç»ŸéŸ³é¢‘-è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å•é˜¶æ®µæˆ–å¤šé˜¶æ®µç®€å•å †å çš„å±€é™ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š

- **æ ‡ç­¾ç³»ç»Ÿé©æ–°**ï¼šç”¨è‡ªç„¶è¯­è¨€æç¤ºæ›¿ä»£å¤æ‚åˆ†å±‚æ ‡ç­¾ï¼Œç¼©å°é¢„è®­ç»ƒä¸å¾®è°ƒé˜¶æ®µçš„åˆ†å¸ƒå·®è·
- **åŒæ¨¡æ€è”åˆè®­ç»ƒ**ï¼šVoice Chat ä¸ Audio Analysis ä¸¤ç§äº¤äº’æ¨¡å¼æ— éœ€ç³»ç»Ÿæç¤ºè¯å³å¯éšå¼åˆ‡æ¢
- **åå¥½å¯¹é½ä¼˜åŒ–**ï¼šå¼•å…¥ DPO é˜¶æ®µç›´æ¥ä¼˜åŒ–äººç±»åå¥½ï¼Œæ˜¾è‘—æå‡äº‹å®å‡†ç¡®æ€§ï¼ˆfactualityï¼‰å’ŒæŒ‡ä»¤éµå¾ªåº¦

è®­ç»ƒæ•°æ®è§„æ¨¡è¾¾ **520k å°æ—¶**ï¼ˆè¯­éŸ³ 370k + éŸ³ä¹ 140k + ç¯å¢ƒå£° 10kï¼‰ï¼Œåœ¨ AIR-Bench è¯„æµ‹ä¸­å–å¾— 7.18 åˆ†ï¼Œè¶…è¶Š Gemini-1.5-proï¼ˆ6.97 åˆ†ï¼‰ã€‚



### Stage_1: Multi-Task Pre-Training

#### æ•°æ®ç­–ç•¥ä¸æ ‡ç­¾ç³»ç»Ÿé©æ–°

**æ ¸å¿ƒæ”¹è¿›ï¼šä»åˆ†å±‚æ ‡ç­¾åˆ°è‡ªç„¶è¯­è¨€æç¤º**

ä¸å‰ä»£ Qwen-Audio ä½¿ç”¨å¤æ‚åˆ†å±‚æ ‡ç­¾ï¼ˆå¦‚ `[Speech Recognition][English]`ï¼‰ä¸åŒï¼ŒQwen2-Audio é‡‡ç”¨**è‡ªç„¶è¯­è¨€æè¿°**ä½œä¸ºä»»åŠ¡æç¤ºï¼š

```diff
- æ—§æ–¹å¼ï¼ˆQwen-Audioï¼‰: [ASR][English] + [Audio][Speech]
+ æ–°æ–¹å¼ï¼ˆQwen2-Audioï¼‰: "Detect the language and recognize the speech: <audio>"
```

**æ•°æ®è§„æ¨¡åˆ†å¸ƒ**ï¼š

| æ¨¡æ€ç±»å‹           | æ—¶é•¿      | å æ¯”  | ä¸»è¦æ¥æº                                    |
| :----------------- | :-------- | :---- | :------------------------------------------ |
| **è¯­éŸ³ (Speech)**  | 370k å°æ—¶ | 71.2% | Librispeechã€Aishell2ã€Common Voiceã€Fleurs |
| **éŸ³ä¹ (Music)**   | 140k å°æ—¶ | 26.9% | MusicCapsã€ç§æœ‰éŸ³ä¹æ•°æ®é›†                   |
| **ç¯å¢ƒå£° (Sound)** | 10k å°æ—¶  | 1.9%  | AudioCapsã€Clothoã€VocalSound               |

**æç¤ºæ¨¡æ¿è®¾è®¡åŸåˆ™**ï¼š

- **ä»»åŠ¡æ˜ç¡®æ€§**ï¼šæ˜ç¡®æŒ‡ç¤ºæ¨¡å‹æ‰§è¡Œ ASRã€S2TTã€AACï¼ˆéŸ³é¢‘å­—å¹•ï¼‰ç­‰ä»»åŠ¡
- **è¯­è¨€é€šç”¨æ€§**ï¼šæ”¯æŒä¸­è‹±å¾·æ³•ç­‰ 7 ç§è¯­è¨€çš„è¯­éŸ³ç¿»è¯‘ä»»åŠ¡
- **é›¶æ ·æœ¬æ³›åŒ–**ï¼šé€šè¿‡å¤šæ ·åŒ–è‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œæå‡æœªè§ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›

#### æ¨¡å‹åˆå§‹åŒ–ä¸è®­ç»ƒç­–ç•¥

**æ¶æ„ç»„ä»¶åˆå§‹åŒ–**ï¼š

- **éŸ³é¢‘ç¼–ç å™¨**ï¼šåŸºäº **Whisper-large-v3** åˆå§‹åŒ–ï¼ˆ32 å±‚ Transformerï¼Œ20 å¤´ï¼Œd_model=1280ï¼‰
- **è¯­è¨€æ¨¡å‹**ï¼šåŸºäº **Qwen-7B** åˆå§‹åŒ–ï¼ˆ32 å±‚ Decoderï¼ŒGQA æ¶æ„ï¼‰
- **æŠ•å½±å±‚**ï¼š**éšæœºåˆå§‹åŒ–**ï¼Œå•å±‚çº¿æ€§å±‚ï¼ˆ1280â†’4096ï¼‰ï¼Œçº¦ 5.2M å‚æ•°

**è®­ç»ƒé…ç½®**ï¼š

- **éŸ³é¢‘é¢„å¤„ç†**ï¼š16kHz é‡‡æ ·ç‡ â†’ 128 é€šé“ Mel-Spectrogramï¼ˆ25ms çª—å£ï¼Œ10ms è·³æ­¥ï¼‰
- **ä¸‹é‡‡æ ·ç­–ç•¥**ï¼šConv1d stride=2ï¼Œå¸§ç‡ä» 50Hz é™è‡³ 25Hzï¼ˆæ¯å¸§å¯¹åº” 40ms éŸ³é¢‘ï¼‰
- **åºåˆ—é•¿åº¦**ï¼šéŸ³é¢‘åºåˆ—çº¦ 750 tokensï¼ˆ30 ç§’éŸ³é¢‘ï¼‰ï¼Œæ–‡æœ¬åºåˆ—æœ€é•¿ 8192 tokens

**æ”¶æ•›æ€§ä¼˜åŒ–**ï¼š

- **é˜¶æ®µå¼è§£å†»**ï¼šåˆæœŸå†»ç»“ Whisper Encoder å’Œ Qwen LLMï¼Œä»…è®­ç»ƒæŠ•å½±å±‚ï¼›åæœŸé€æ­¥è§£å†» Encoder é¡¶å±‚
- **å­¦ä¹ ç‡ç­–ç•¥**ï¼šæŠ•å½±å±‚ä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡ï¼ˆ1e-4ï¼‰ï¼Œé¢„è®­ç»ƒç»„ä»¶ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼ˆ1e-5ï¼‰ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜

### Stage_2: Supervised Fine-Tuning

#### åŒæ¨¡æ€äº¤äº’æ¶æ„è®¾è®¡

Qwen2-Audio çš„ SFT é˜¶æ®µæ ¸å¿ƒåˆ›æ–°æ˜¯**ç»Ÿä¸€åŒæ¨¡æ€è®­ç»ƒ**ï¼Œæ¶ˆé™¤æ˜¾å¼æ¨¡å¼åˆ‡æ¢ï¼š

##### Audio Analysis æ¨¡å¼

- **è¾“å…¥æ„æˆ**ï¼šéŸ³é¢‘ + æ–‡æœ¬æŒ‡ä»¤ï¼ˆå¦‚ "What's the mood of the speaker?"ï¼‰

- **åº”ç”¨åœºæ™¯**ï¼šç¦»çº¿éŸ³é¢‘æ–‡ä»¶åˆ†æã€ç‰¹å®šä»»åŠ¡æ‰§è¡Œï¼ˆASRã€æƒ…æ„Ÿè¯†åˆ«ã€ç¿»è¯‘ï¼‰

- **æ•°æ®æ ¼å¼**ï¼š

  ```json
  {
    "audio": "path/to/audio.wav",
    "conversations": [
      {"from": "human", "value": "<audio>\nWhat's the mood of the speaker?"},
      {"from": "gpt", "value": "The speaker sounds anxious and stressed."}
    ]
  }
  ```

##### Voice Chat æ¨¡å¼

- **è¾“å…¥æ„æˆ**ï¼šä»…éŸ³é¢‘æµï¼ˆç”¨æˆ·è¯­éŸ³æé—®ï¼‰
- **åº”ç”¨åœºæ™¯**ï¼šåœ¨çº¿è‡ªç”±å¯¹è¯ã€è¯­éŸ³åŠ©æ‰‹ã€å¤šè½®é—²èŠ
- **æ•°æ®ç‰¹å¾**ï¼šåŒ…å«è‡ªç„¶å¯¹è¯éŸµå¾‹ã€æ‰“æ–­ã€è¯­æ°”è¯ç­‰éæ–‡æœ¬ä¿¡æ¯
- **ç‰¹æ®Šå¤„ç†**ï¼šä½¿ç”¨ `<|audio_bos|>` å’Œ `<|audio_eos|>` æ ‡è®°éŸ³é¢‘è¾¹ç•Œ

#### æ•°æ®è´¨é‡ä¸å¤šæ ·æ€§æ§åˆ¶

**è´¨é‡æ§åˆ¶æœºåˆ¶**ï¼š

1. **äººå·¥ç­›é€‰**ï¼šå‰”é™¤ä½è´¨é‡ã€å« PIIï¼ˆä¸ªäººèº«ä»½ä¿¡æ¯ï¼‰çš„æ ·æœ¬
2. **å¤æ‚åº¦åˆ†å±‚**ï¼šç®€å•æŒ‡ä»¤ï¼ˆ"Transcribe this"ï¼‰ä¸å¤æ‚æ¨ç†ï¼ˆ"Analyze the emotional progression"ï¼‰æŒ‰ 3:7 é…æ¯”
3. **é•¿åº¦å‡è¡¡**ï¼šçŸ­éŸ³é¢‘ï¼ˆ<30sï¼‰ä¸é•¿éŸ³é¢‘ï¼ˆ2-5minï¼‰æ··åˆï¼Œé˜²æ­¢é•¿åº¦åè§

**æ•°æ®å¢å¼ºç­–ç•¥**ï¼š

- **éŸ³é¢‘å¢å¼º**ï¼šæ·»åŠ èƒŒæ™¯å™ªå£°ï¼ˆSNR 5-20dBï¼‰ã€é€Ÿåº¦æ‰°åŠ¨ï¼ˆ0.9x-1.1xï¼‰ã€éŸ³é‡å˜åŒ–
- **æ–‡æœ¬å¢å¼º**ï¼šåŒä¸€éŸ³é¢‘é…å¤šä¸ªä¸åŒè¡¨è¿°çš„æŒ‡ä»¤ï¼Œæå‡æŒ‡ä»¤éµå¾ªé²æ£’æ€§



### Stage_3: Direct Preference Optimization(DPO)

#### DPO ç®—æ³•é€‚é…

Qwen2-Audio å¼•å…¥ **DPO** ä½œä¸ºç¬¬ä¸‰ä¼˜åŒ–é˜¶æ®µï¼ŒåŒºåˆ«äºä¼ ç»Ÿ RLHF+PPO çš„å¤æ‚æµç¨‹ï¼š

**æ ¸å¿ƒå…¬å¼**ï¼š

```math
\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
```

å…¶ä¸­ï¼š

- *x* ï¼šè¾“å…¥ï¼ˆéŸ³é¢‘ + å¯é€‰æ–‡æœ¬ï¼‰
- *y**w* ï¼šäººç±»åå¥½çš„ä¼˜è´¨å›å¤ï¼ˆChosenï¼‰
- *y**l* ï¼šè¾ƒå·®çš„å›å¤ï¼ˆRejectedï¼‰
- *Ï€*ref ï¼šå‚è€ƒæ¨¡å‹ï¼ˆSFT é˜¶æ®µæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼Œå†»ç»“å‚æ•°ï¼‰
- *Î²* ï¼šæ¸©åº¦ç³»æ•°ï¼ˆé€šå¸¸ 0.1-0.5ï¼‰ï¼Œæ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦

####  åå¥½æ•°æ®æ„å»º

**æ•°æ®æ”¶é›†ç»´åº¦**ï¼š

| ä¼˜åŒ–ç»´åº¦       | Chosen ç‰¹å¾                            | Rejected ç‰¹å¾            | æ•°æ®å æ¯” |
| :------------- | :------------------------------------- | :----------------------- | :------- |
| **äº‹å®å‡†ç¡®æ€§** | æ­£ç¡®è¯†åˆ«éŸ³é¢‘å†…å®¹ã€æ•°å­—ã€äººå           | å¹»è§‰ã€é”™è¯¯è½¬å½•ã€æ·»æ²¹åŠ é†‹ | 40%      |
| **æŒ‡ä»¤éµå¾ª**   | ä¸¥æ ¼æŒ‰æŒ‡ä»¤æ ¼å¼è¾“å‡ºï¼ˆå¦‚JSONã€æŒ‡å®šå­—æ•°ï¼‰ | åç¦»æŒ‡ä»¤ã€æ ¼å¼é”™è¯¯       | 30%      |
| **å®‰å…¨æ€§**     | æ‹’ç»å›ç­”æ•æ„Ÿé—®é¢˜ï¼ˆå¦‚æš´åŠ›æ•™å”†ï¼‰         | æä¾›æœ‰å®³æˆ–ä¸å®‰å…¨å†…å®¹     | 20%      |
| **å¸®åŠ©æ€§**     | è¯¦ç»†ã€æœ‰å»ºè®¾æ€§çš„åˆ†æ                   | è¿‡äºç®€çŸ­ã€æ•·è¡çš„å›ç­”     | 10%      |

**éŸ³é¢‘ç‰¹å®šæŒ‘æˆ˜**ï¼š

- **å£°å­¦æ··æ·†**ï¼šåœ¨å˜ˆæ‚ç¯å¢ƒä¸­ï¼ŒChosen åº”æ­£ç¡®è¯†åˆ«å†…å®¹ï¼ŒRejected å¯èƒ½å—å™ªå£°å¹²æ‰°äº§ç”Ÿè¯¯å¬
- **è¯­æ°”ç†è§£**ï¼šå¯¹åŒä¸€å¥è¯ï¼ŒChosen è¯†åˆ«è®½åˆº/çœŸè¯šè¯­æ°”ï¼ŒRejected è¯¯åˆ¤


## vLLM Deploy

- å‚è€ƒï¼š[vllm/README.md](./vllm/README.md)



## Evaluation
### Dataset Description

|        æ•°æ®é›†åç§°        | ä»»åŠ¡ç±»å‹ |       è¯­è¨€/è§„æ¨¡       | æ•°æ®æè¿°                                    | å…¸å‹ç”¨é€”                        |                                   èµ„æºé“¾æ¥                                   |
| :-----------------: | :--: | :---------------: | :-------------------------------------- | :-------------------------- | :----------------------------------------------------------------------: |
|   **LibriSpeech**   |  ASR (Automatic Speech Recognition) |   è‹±è¯­<br>~1000å°æ—¶   | å¼€æºæœ‰å£°ä¹¦è¯­éŸ³ï¼Œ16kHzï¼ŒåŒ…å«ä¸åŒæ¸…æ™°åº¦ï¼ˆclean/otherï¼‰çš„æœ—è¯»è¯­éŸ³ | è‹±è¯­è¯­éŸ³è¯†åˆ«åŸºçº¿æµ‹è¯•ã€æœ‰å£°ä¹¦ ASR          |                   [OpenSLR](https://www.openslr.org/12)                  |
| **Common Voice 15** |  ASR |  å¤šè¯­è¨€<br>(100+è¯­ç§)  | Mozillaä¼—åŒ…è¯­éŸ³ï¼Œæ¶µç›–å¤šæ ·åŒ–å£éŸ³ã€å¹´é¾„ã€æ€§åˆ«ï¼ŒCC0å¼€æºåè®®       | å¤šè¯­è¨€ ASRã€ä½èµ„æºè¯­è¨€ç ”ç©¶ã€å£éŸ³é²æ£’æ€§       | [HuggingFace](https://huggingface.co/datasets/fsicoli/common_voice_15_0) |
|      **FLEURS**     |  ASR | 102ç§è¯­è¨€<br>~10h/è¯­ç§ | åŸºäºFLoRES-200çš„è¯­éŸ³ç‰ˆæœ¬ï¼Œå¥å­çº§å¯¹é½ï¼Œè¦†ç›–å¹¿æ³›è¯­ç³»          | å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€è·¨è¯­è¨€è¿ç§»å­¦ä¹ ã€å°æ ·æœ¬è¯„ä¼°       |       [HuggingFace](https://huggingface.co/datasets/google/fleurs)       |
|     **CoVoST 2**    | S2TT |  å¤šè¯­è¨€å¯¹<br>(å¦‚enâ†’de) | åŸºäºCommon Voiceçš„è¯­éŸ³-ç¿»è¯‘æ–‡æœ¬å¹³è¡Œè¯­æ–™ï¼Œæ”¯æŒç«¯åˆ°ç«¯ç¿»è¯‘      | è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ˆS2TTï¼‰ã€çº§è” vs ç«¯åˆ°ç«¯ç¿»è¯‘å¯¹æ¯” |      [HuggingFace](https://hf-mirror.com/datasets/fixie-ai/covost2)      |
|       **MELD**      |  SER |   è‹±è¯­<br>1,400+å¯¹è¯  | æºè‡ªã€Šè€å‹è®°ã€‹çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ ‡æ³¨7ç§æƒ…æ„Ÿç±»åˆ«ï¼ˆå«éŸ³é¢‘ã€æ–‡æœ¬ã€è§†é¢‘ï¼‰       | è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€å¯¹è¯æƒ…æ„Ÿç†è§£       |                  [å®˜ç½‘](https://affective-meld.github.io/)                 |
|    **VocalSound**   |  VSC |  è‹±è¯­<br>~21,000æ ·æœ¬  | 6ç±»äººå£°éŸ³æ•ˆï¼ˆç¬‘ã€å’³ã€å–·åšç­‰ï¼‰ï¼Œä¸“æ³¨éè¯­è¨€äººå£°äº‹ä»¶               | äººå£°éŸ³æ•ˆåˆ†ç±»ã€è¯­éŸ³äº‹ä»¶æ£€æµ‹ã€å¥åº·ç›‘æµ‹ï¼ˆå’³å—½æ£€æµ‹ï¼‰    |            [GitHub](https://github.com/YuanGongND/vocalsound)            |


### Data Download

**FROM**: https://github.com/QwenLM/Qwen2-Audio/blob/main/eval_audio/EVALUATION.md

ä¾æ®å®˜æ–¹æ•™ç¨‹ä¸‹è½½ç›¸åº”çš„æ•°æ®åŠ**è¯„æµ‹æ¸…å• jsonl**

#### ASR

- **Data url**

  - LibriSpeech
    - å®˜æ–¹é“¾æ¥ï¼šhttps://www.openslr.org/12

  - Common Voice 15
    - HuggingFaceï¼šhttps://huggingface.co/datasets/fsicoli/common_voice_15_0

  - FLEURS
    - HuggingFaceï¼šhttps://huggingface.co/datasets/google/fleurs

- **Eval list**

  - LibriSpeechï¼ˆASRï¼‰

    - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/librispeech_eval.jsonl

    - Common Voice 15ï¼ˆASRï¼‰
      - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/cv15_asr_en_eval.jsonl
      - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/cv15_asr_zh_eval.jsonl
      - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/cv15_asr_yue_eval.jsonl
      - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/cv15_asr_fr_eval.jsonl

  - FLEURSï¼ˆASRï¼‰
    - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/fleurs_asr_zh_eval.jsonl

####  S2TT

- **Data url**
  - CoVoST 2
    - HuggingFaceï¼ˆmirrorï¼‰ï¼šhttps://hf-mirror.com/datasets/fixie-ai/covost2

- **Eval list**
  - CoVoST 2ï¼ˆS2TTï¼‰ 
    - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/covost2_eval.jsonl

#### **SER**

- **Data url**
  - MELD
    - å®˜æ–¹é“¾æ¥ï¼šhttps://affective-meld.github.io/

- **Eval list**
  - MELDï¼ˆSERï¼‰
    - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/meld_eval.jsonl

#### **VSC**

- **Data url**
  - VocalSound
    - GitHubï¼šhttps://github.com/YuanGongND/vocalsound

- **Eval list**
  - VocalSoundï¼ˆVSCï¼‰
    - https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/evaluation/vocalsound_eval.jsonl


### Run Scripts

- **ç¡®ä¿æ•°æ®é›†å‡å·²ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„ä¸‹**ï¼Œå‚è€ƒå®˜æ–¹è¯„ä¼°è¯´æ˜ï¼šhttps://github.com/QwenLM/Qwen2-Audio/blob/main/eval_audio/EVALUATION.md
- åŸºäº Transformers ç²¾åº¦æµ‹è¯•
  - å‚è€ƒ:  [transformers/EVALUATION.md](./transformers/EVALUATION.md)
- åŸºäº VLLM ç²¾åº¦æµ‹è¯•
  - å‚è€ƒ:  [vllm/EVALUATION.md](./vllm/EVALUATION.md)


### Test Result

- å®˜æ–¹ç²¾åº¦æµ‹è¯•ç»“æœï¼šhttps://github.com/QwenLM/Qwen2-Audio/blob/main/README.md#evaluation


| **Task** | **Dataset**     | **Split**  | **Count** | **Metric** | Official Score | Transformers Score | VLLM Score |
| -------- | --------------- | ---------- | --------- | ---------- | ---------------------------- | ----------------------- | --------------- |
| ASR      | Librispeech     | dev_clean  | 2694      | WER        | 1.7                          | 1.68                    | 2.24            |
|          |                 | dev_other  | 2857      |            | 3.6                          | 3.65                    | 4.41            |
|          |                 | test_clean | 2611      |            | 1.7                          | 1.70                    | 2.24            |
|          |                 | test_other | 2932      |            | 4.0                          | 4.03                    | 4.69            |
|          | Fleurs          | test_zh    | 944       |            | 7.0                          | 7.01                    | 7.33            |
|          | Common Voice 15 | test_zh    | 10625     |            | 6.5                          | 6.89                    | 6.62            |
|          |                 | test_yue   | 5593      |            | 5.9                          | 5.87                    | 6.06            |
|          |                 | test_fr    | 16132     |            | 9.6                          | 9.55                    | 9.60            |
|          |                 | test_en    | 16381     |            | 8.7                          | 8.76                    | 9.72            |
| S2TT     | CoVoST2         | en_zh      | 30984     | BLEU       | 45.6                         | 45.5                    | 45.6            |
|          |                 | en_de      | 30883     |            | 29.6                         | 29.6                    | 29.8            |
|          |                 | de_en      | 27017     |            | 33.6                         | 33.6                    | 35.4            |
|          |                 | zh_en      | 9741      |            | 24.0                         | 23.9                    | 24.7            |
| SER      | Meld            | test+dev   | 3716      | ACC        | 0.535                        | 0.541                   | 0.548           |
| VSC      | VocalSound      | test+valid | 5446      | ACC        | 0.9395                       | 0.9329                  | 0.9342          |

